import pandas as pd
import numpy as np
import pickle
import os
import ast
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from collections import Counter

# Import ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÄ‡∏î‡∏¥‡∏°
from clean_cosmetic_ingredients import clean_ingredients
from model_training import train_from_scratch

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå
DATA_DIR = "data"
OUTPUT_DIR = "model_output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

def find_optimal_k(X, max_k=10):
    """
    ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£: Silhouette Method
    ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ K (‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°) ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÇ‡∏î‡∏¢‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏Å‡∏•‡πâ‡∏ä‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏° 
    ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏° (‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏Å‡∏•‡πâ 1 ‡∏Ñ‡∏∑‡∏≠‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î)
    """
    n_samples = X.shape[0]
    if n_samples < 2:
        return 1
        
    actual_max_k = min(max_k, n_samples - 1)
    best_k = 2
    best_score = -1
    
    print(f"üîç Finding optimal K (range 2 to {actual_max_k})...")
    
    for k in range(2, actual_max_k + 1):
        km = KMeans(n_clusters=k, n_init=10, random_state=42)
        labels = km.fit_predict(X)
        score = silhouette_score(X, labels)
        
        if score > best_score:
            best_score = score
            best_k = k
            
    print(f"‚úÖ Optimal K found: {best_k} (Silhouette Score: {best_score:.4f})")
    return best_k

def run_pipeline():
    # 1. Load Data
    cleaned_csv = os.path.join(DATA_DIR, "cosmetics_cleaned_final.csv")
    if not os.path.exists(cleaned_csv):
        print(f"‚ùå Error: {cleaned_csv} not found.")
        return

    df = pd.read_csv(cleaned_csv)
    print(f"üìÇ Loaded {len(df)} products for training.")

    # 2. Vectorization (TF-IDF)
    # ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£: TF-IDF ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ö‡πà‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡πÄ‡∏ä‡πà‡∏ô Water) ‡∏î‡πâ‡∏ß‡∏¢ max_df=0.8
    # ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ú‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (Noise) ‡∏î‡πâ‡∏ß‡∏¢ min_df=2
    vectorizer = TfidfVectorizer(max_df=0.8, min_df=2) 
    X = vectorizer.fit_transform(df['clean_ingredients'].fillna(''))

    # 3. K-Means with Statistical Optimal K
    k = find_optimal_k(X)
    if k > 1:
        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)
        df['cluster'] = kmeans.fit_predict(X)
    else:
        df['cluster'] = 0
        print("‚ö†Ô∏è Not enough data for clustering. Assigned all to cluster 0.")

    # 4. Generate Cluster Profiles
    cluster_profile = {}
    feature_names = np.array(vectorizer.get_feature_names_out())

    for i in range(k if k > 1 else 1):
        cluster_products = df[df['cluster'] == i]
        
        # ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Skin Type (‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å String ‡πÄ‡∏õ‡πá‡∏ô List)
        all_skin_types = []
        for st in cluster_products['skin_type'].dropna():
            try:
                # ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö format ['oily', 'dry'] ‡∏´‡∏£‡∏∑‡∏≠ oily, dry
                st_str = str(st)
                if '[' in st_str:
                    all_skin_types.extend(ast.literal_eval(st_str))
                else:
                    all_skin_types.extend([s.strip() for s in st_str.split(',') if s.strip()])
            except:
                continue
        
        # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏™‡∏†‡∏≤‡∏û‡∏ú‡∏¥‡∏ß‡∏ó‡∏µ‡πà‡πÇ‡∏î‡∏î‡πÄ‡∏î‡πà‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ô‡∏±‡πâ‡∏ô 2 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
        top_skin_types = [st for st, count in Counter(all_skin_types).most_common(2)]
        
        # ‡∏´‡∏≤ Key Ingredients (‡∏™‡πà‡∏ß‡∏ô‡∏ú‡∏™‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏ó‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏°) ‡∏à‡∏≤‡∏Å Centroid
        if k > 1:
            centroid = kmeans.cluster_centers_[i]
            top_indices = centroid.argsort()[-10:][::-1]
            key_ingredients = feature_names[top_indices].tolist()
        else:
            key_ingredients = feature_names[X.toarray().mean(axis=0).argsort()[-10:][::-1]].tolist()
        
        cluster_profile[i] = {
            "dominant_skin_types": top_skin_types if top_skin_types else ["all skin types"],
            "key_ingredients": key_ingredients,
            "product_count": len(cluster_products)
        }

    # 5. Save Model Output
    profile_path = os.path.join(OUTPUT_DIR, "cluster_profile.pkl")
    with open(profile_path, "wb") as f:
        pickle.dump(cluster_profile, f)
    print(f"üíæ Cluster profile saved to {profile_path}")

    # 6. Train Association Rules ‡∏ï‡πà‡∏≠
    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ú‡∏™‡∏° (Association Rules)
    # ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Entity Resolution ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß
    print("üîÑ Training association rules...")
    train_from_scratch(df, cleaned_csv=cleaned_csv)
    
    print("üöÄ Full pipeline completed successfully!")

if __name__ == "__main__":
    run_pipeline()